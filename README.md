# LLMs-Emergence-Mechanism

### **Paper Title**

Cognitive Activation and Chaotic Dynamics in Large Language Models: A Quasi-Lyapunov Analysis of Reasoning Mechanisms

### Key Contributions

- Proposing the concept of "Cognitive Activation" to describe the dynamic information extraction process in large language models.
- Establishing a parameter space correlation analysis framework to elucidate the information flow mechanism between model layers.
- Introducing quasi-Lyapunov exponents to quantitatively analyze model reasoning properties from a chaos theory perspective.
- Proposing a chaos-based functional partitioning method for large language models, distinguishing between convergence and divergence domains.

### Theoretical Framework

- Cognitive Activation Theory: Proposes a dynamic information extraction mechanism, revealing that Large Language Models (LLMs) achieve reasoning capabilities through context-dependent parameter activation, with the core being the input-driven dynamic reorganization process in the parameter space.

- Chaotic Dynamics Modeling: Quantifies the chaotic characteristics of model reasoning based on the Quasi-Lyapunov Exponent (QLE).

### **Authors**
All authors of this paper are from the School of Management at Hefei University of Technology, dedicated to exploring the emergence mechanisms of large language models.

### **Notes**
Reproducible code and sample data are currently being organized, and the paper is being uploaded to Arxiv, with continuous updates to follow.

### **Cite this paper**
@article{
  title={Cognitive Activation and Chaotic Dynamics in Large Language Models: A Quasi-Lyapunov Analysis of Reasoning Mechanisms},
  author={Xiaojian Li, Yongkang Leng, Ruiqing Ding, Hangjie Mo, Shanlin Yang},
  journal={https://github.com/HFUT-emergence/LLMs-Emergence-Mechanism},
  year={2025}
}
